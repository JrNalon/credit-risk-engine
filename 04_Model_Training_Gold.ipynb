{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce91191c-a859-4014-847a-46afb0be5232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Célula 1: Preparação dos Vetores\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# 1. Ler a tabela Silver\n",
    "df = spark.read.table(\"portfolio_credit.german_credit_silver\")\n",
    "\n",
    "# 2. Definir quem são as colunas de entrada (Features)\n",
    "feature_cols = [\n",
    "    \"duration\", \"credit_amount\", \"monthly_installment\", \"age\",  # Numéricas\n",
    "    \"checking_status_ix\", \"credit_history_ix\", \"purpose_ix\", \"housing_ix\" # Categóricas (Indexadas)\n",
    "]\n",
    "\n",
    "# 3. Criar o Vetor de Features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_model = assembler.transform(df)\n",
    "\n",
    "# Selecionar apenas o Vetor de Features e a Resposta (Label)\n",
    "model_data = df_model.select(\"features\", \"label\")\n",
    "\n",
    "print(\"Dados vetorizados e prontos para o algoritmo.\")\n",
    "display(model_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cdf443-068f-4de0-b9f5-505ddfd7a963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Célula 2: Split, Balanceamento e Treinamento (Versão Otimizada)\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# 1. Divisão dos dados (70% Treino, 30% Teste)\n",
    "train_data, test_data = model_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# --- TÉCNICA AVANÇADA: Balanceamento de Classes ---\n",
    "# O problema: Temos muito mais \"Bons\" (0) do que \"Ruins\" (1).\n",
    "# A solução: Vamos dar um \"peso\" maior para o erro na classe 1.\n",
    "\n",
    "# Contamos quantos tem de cada no treino\n",
    "dataset_size = train_data.count()\n",
    "num_boms = train_data.filter(col(\"label\") == 0).count()\n",
    "num_ruins = train_data.filter(col(\"label\") == 1).count()\n",
    "\n",
    "# Calculamos o peso. Se temos 3x menos ruins, o peso deles será 3x maior.\n",
    "balancing_ratio = num_boms / num_ruins\n",
    "print(f\"Ratio de Balanceamento: Cada erro de 'Mau Pagador' vale por {balancing_ratio:.2f} erros de 'Bom Pagador'.\")\n",
    "\n",
    "# Criamos a coluna de pesos no dataset de Treino\n",
    "train_data_weighted = train_data.withColumn(\"classWeight\", \n",
    "                                            when(col(\"label\") == 1, balancing_ratio)\n",
    "                                            .otherwise(1.0))\n",
    "\n",
    "# 2. Criar e Treinar o Modelo (Random Forest Turbinado)\n",
    "# weightCol: Usa o peso que calculamos\n",
    "# numTrees: Aumentamos de 50 para 100 (Mais especialistas opinando)\n",
    "# maxDepth: Aumentamos de 5 para 10 (Árvores mais profundas/inteligentes)\n",
    "# subsamplingRate: Usa 80% dos dados para cada árvore (evita vício)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\", \n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"classWeight\",  # <--- O Segredo do Balanceamento\n",
    "    numTrees=100,             # Aumentamos\n",
    "    maxDepth=10,              # Aumentamos\n",
    "    subsamplingRate=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Treinando o modelo otimizado... (Pode demorar um pouquinho mais)\")\n",
    "rf_model = rf.fit(train_data_weighted)\n",
    "\n",
    "print(\"Modelo treinado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4189c0-d6e2-4251-b7b4-e673006d7ee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Célula 3: Avaliação do Modelo\n",
    "\n",
    "# 1. Fazer previsões nos dados de Teste (que o modelo nunca viu)\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# 2. Avaliar a performance\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"--- RESULTADO DO MODELO ---\")\n",
    "print(f\"AUC (Area Under Curve): {auc:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Ver algumas previsões (prediction = 0 é Bom, 1 é Ruim)\n",
    "display(predictions.select(\"label\", \"prediction\", \"probability\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fddebdb-df87-4f50-bfd1-5d484604bb47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Célula 4: Quais variáveis pesaram mais?\n",
    "import pandas as pd\n",
    "\n",
    "# Extrair a importância das variáveis do modelo treinado\n",
    "importances = rf_model.featureImportances\n",
    "\n",
    "# Criar um DataFrame simples para visualizar\n",
    "feature_importance_df = pd.DataFrame(list(zip(feature_cols, importances)), columns=[\"Feature\", \"Importance\"])\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Mostrar gráfico\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df, palette=\"viridis\")\n",
    "plt.title(\"Importância das Variáveis no Modelo de Crédito\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c9188ba-0473-426f-bce6-bda6b5497552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Célula 5: Gerar Tabela Gold (Base completa com Scores)\n",
    "\n",
    "# Aplicar modelo na base inteira (simulando score de produção)\n",
    "full_predictions = rf_model.transform(df_model)\n",
    "\n",
    "# Vamos explodir o vetor de probabilidade para pegar só a chance de ser \"Ruim\" (índice 1)\n",
    "# Isso facilita muito para quem vai consumir a tabela depois\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import element_at\n",
    "\n",
    "# Transforma a coluna vetorial 'probability' em array e pega o segundo elemento (Probabilidade de Default)\n",
    "df_gold = full_predictions.withColumn(\"score_risco\", element_at(vector_to_array(\"probability\"), 2))\n",
    "df_gold = df_gold.select(\"label\", \"prediction\", \"score_risco\", *feature_cols) \n",
    "\n",
    "# Salvar\n",
    "table_name = \"portfolio_credit.german_credit_gold_predictions\"\n",
    "df_gold.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"Tabela Gold com Scores salva: {table_name}\")\n",
    "display(df_gold)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Model_Training_Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
